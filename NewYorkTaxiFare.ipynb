{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e395d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML\\\\NewYorkTaxi_Jupyter'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2042a317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2009-06-15 17:26:21 UTC\n",
      "1    2010-01-05 16:52:16 UTC\n",
      "2    2011-08-18 00:35:00 UTC\n",
      "3    2012-04-21 04:30:42 UTC\n",
      "4    2010-03-09 07:51:00 UTC\n",
      "Name: pickup_datetime, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training = pd.read_csv('data/train.csv', nrows=100)\n",
    "test = pd.read_csv('data/test.csv', nrows=100)\n",
    "\n",
    "pd.set_option('display.max_columns',10)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "print(training.pickup_datetime.head())\n",
    "type(training)\n",
    "\n",
    "training.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f388f2c",
   "metadata": {},
   "source": [
    "We need to write a custom function for cross validation.\n",
    "\n",
    "Ideas:\n",
    "0- Check for empty and null values\n",
    "1- Simplify pick time to a number between 0 to 24 to just represent time of the day\n",
    "2- Come up with a new column that shows the distance. This may not be great since the exact locations may affect the length of the trip that could affect the fare.\n",
    "3- Check for linear correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9dd7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Read 10000 training rows\n",
      "Read 9914 test rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training = pd.read_csv('data/train.csv', on_bad_lines='skip') # TODO remove limit\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "print(\"Read \" + str(len(training)) + \" training rows\")\n",
    "print(\"Read \" + str(len(test)) + \" test rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a7d76",
   "metadata": {},
   "source": [
    "Now we break apart the pickup times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "346dc52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Read 1000000 training rows\n",
      "Read 9914 test rows\n",
      "   fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "0          4.5        -73.844311        40.721319         -73.841610         40.712278   \n",
      "1         16.9        -74.016048        40.711303         -73.979268         40.782004   \n",
      "2          5.7        -73.982738        40.761270         -73.991242         40.750562   \n",
      "3          7.7        -73.987130        40.733143         -73.991567         40.758092   \n",
      "4          5.3        -73.968095        40.768008         -73.956655         40.783762   \n",
      "\n",
      "   passenger_count  pickup_year  pickup_month  pickup_hour  pickup_dayofweek  distance  \n",
      "0                1         2009             6           17                 0  1.030764  \n",
      "1                1         2010             1           16                 1  8.450134  \n",
      "2                2         2011             8            0                 3  1.389525  \n",
      "3                1         2012             4            4                 5  2.799270  \n",
      "4                1         2010             3            7                 1  1.999157  \n",
      "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
      "0        -73.973320        40.763805         -73.981430         40.743835                1   \n",
      "1        -73.986862        40.719383         -73.998886         40.739201                1   \n",
      "2        -73.982524        40.751260         -73.979654         40.746139                1   \n",
      "3        -73.981160        40.767807         -73.990448         40.751635                1   \n",
      "4        -73.966046        40.789775         -73.988565         40.744427                1   \n",
      "\n",
      "   pickup_year  pickup_month  pickup_hour  pickup_dayofweek  distance  \n",
      "0         2015             1           13                 1  2.323260  \n",
      "1         2015             1           13                 1  2.425353  \n",
      "2         2011            10           11                 5  0.618628  \n",
      "3         2012            12           21                 5  1.961033  \n",
      "4         2012            12           21                 5  5.387301  \n"
     ]
    }
   ],
   "source": [
    "from math import cos, asin, sqrt, pi\n",
    "import pandas as pd\n",
    "\n",
    "training = pd.read_csv('data/train.csv', on_bad_lines='skip', nrows=1000000) # TODO remove limit\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "print(\"Read \" + str(len(training)) + \" training rows\")\n",
    "print(\"Read \" + str(len(test)) + \" test rows\")\n",
    "\n",
    "def convert_pickup_times(df):\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC', errors='coerce')\n",
    "\n",
    "def break_pickup_times_to_components(df):\n",
    "    df['pickup_year'] = df['pickup_datetime'].dt.year\n",
    "    df['pickup_month'] = df['pickup_datetime'].dt.month\n",
    "    #df['pickup_day'] = df['pickup_datetime'].dt.day\n",
    "    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "    #df['pickup_minute'] = df['pickup_datetime'].dt.minute\n",
    "    #converting seconds sounds like an overkill so I won't use it\n",
    "    \n",
    "    #but day of week sounds like a predictive feature\n",
    "    df['pickup_dayofweek'] = df['pickup_datetime'].dt.dayofweek\n",
    "    \n",
    "    #remove the redundant col\n",
    "    df.drop(columns=['pickup_datetime'], inplace=True)\n",
    "    \n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return 12742 * asin(sqrt(a)) #2*R*asin...\n",
    "    \n",
    "def get_distance(row):\n",
    "    return distance(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude'])\n",
    "    \n",
    "def prepare(df):\n",
    "    convert_pickup_times(df)\n",
    "    break_pickup_times_to_components(df)\n",
    "    df.drop(columns=['key'], inplace=True)\n",
    "    df['distance'] = df.apply(get_distance, axis=1)\n",
    "    \n",
    "prepare(training)\n",
    "prepare(test)\n",
    "\n",
    "pd.set_option('display.max_columns',16)\n",
    "pd.set_option('display.width', 100)\n",
    "print(training.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb4d1c",
   "metadata": {},
   "source": [
    "Since the number of cells with null value is relatively small and there's no null values in the test data we can just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2249805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "999990 rows\n"
     ]
    }
   ],
   "source": [
    "def check_for_nulls(df):\n",
    "    is_null = df.isnull()\n",
    "    row_has_null = is_null.any(axis=1)\n",
    "    print(row_has_null.sum())\n",
    "    \n",
    "check_for_nulls(training)\n",
    "check_for_nulls(test)\n",
    "\n",
    "training = training[~training.isnull().any(axis=1)]\n",
    "print(str(len(training)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc380a40",
   "metadata": {},
   "source": [
    "Now, we want to take a look at the correlations between different features and our target variable, i.e. fare amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6c1f6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fare_amount          1.000000\n",
       "pickup_longitude     0.008744\n",
       "pickup_latitude     -0.007680\n",
       "dropoff_longitude    0.009644\n",
       "dropoff_latitude    -0.007629\n",
       "passenger_count      0.012818\n",
       "pickup_year          0.115828\n",
       "pickup_month         0.025104\n",
       "pickup_hour         -0.018935\n",
       "pickup_dayofweek     0.002676\n",
       "distance             0.024779\n",
       "Name: fare_amount, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[training.columns[:]].corr(method='pearson')[:]['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e752822e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['fair_amount'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfair_amount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\plotting\\_core.py:226\u001b[0m, in \u001b[0;36mhist_frame\u001b[1;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, backend, legend, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03mMake a histogram of the DataFrame's columns.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    >>> hist = df.hist(bins=3)\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m plot_backend \u001b[38;5;241m=\u001b[39m _get_plot_backend(backend)\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_backend\u001b[38;5;241m.\u001b[39mhist_frame(\n\u001b[0;32m    227\u001b[0m     data,\n\u001b[0;32m    228\u001b[0m     column\u001b[38;5;241m=\u001b[39mcolumn,\n\u001b[0;32m    229\u001b[0m     by\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m    230\u001b[0m     grid\u001b[38;5;241m=\u001b[39mgrid,\n\u001b[0;32m    231\u001b[0m     xlabelsize\u001b[38;5;241m=\u001b[39mxlabelsize,\n\u001b[0;32m    232\u001b[0m     xrot\u001b[38;5;241m=\u001b[39mxrot,\n\u001b[0;32m    233\u001b[0m     ylabelsize\u001b[38;5;241m=\u001b[39mylabelsize,\n\u001b[0;32m    234\u001b[0m     yrot\u001b[38;5;241m=\u001b[39myrot,\n\u001b[0;32m    235\u001b[0m     ax\u001b[38;5;241m=\u001b[39max,\n\u001b[0;32m    236\u001b[0m     sharex\u001b[38;5;241m=\u001b[39msharex,\n\u001b[0;32m    237\u001b[0m     sharey\u001b[38;5;241m=\u001b[39msharey,\n\u001b[0;32m    238\u001b[0m     figsize\u001b[38;5;241m=\u001b[39mfigsize,\n\u001b[0;32m    239\u001b[0m     layout\u001b[38;5;241m=\u001b[39mlayout,\n\u001b[0;32m    240\u001b[0m     legend\u001b[38;5;241m=\u001b[39mlegend,\n\u001b[0;32m    241\u001b[0m     bins\u001b[38;5;241m=\u001b[39mbins,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    243\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\hist.py:472\u001b[0m, in \u001b[0;36mhist_frame\u001b[1;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, legend, **kwds)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(column, (\u001b[38;5;28mlist\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, ABCIndex)):\n\u001b[0;32m    471\u001b[0m         column \u001b[38;5;241m=\u001b[39m [column]\n\u001b[1;32m--> 472\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# GH32590\u001b[39;00m\n\u001b[0;32m    474\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mselect_dtypes(\n\u001b[0;32m    475\u001b[0m     include\u001b[38;5;241m=\u001b[39m(np\u001b[38;5;241m.\u001b[39mnumber, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime64\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetimetz\u001b[39m\u001b[38;5;124m\"\u001b[39m), exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimedelta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['fair_amount'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training.hist(column='fair_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d6d33d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999990, 4)\n",
      "[[1.03076393e+00 1.00000000e+00 1.70000000e+01 2.00900000e+03]\n",
      " [8.45013360e+00 1.00000000e+00 1.60000000e+01 2.01000000e+03]\n",
      " [1.38952523e+00 2.00000000e+00 0.00000000e+00 2.01100000e+03]\n",
      " ...\n",
      " [1.76174086e+00 5.00000000e+00 1.40000000e+01 2.01300000e+03]\n",
      " [1.84268322e+00 1.00000000e+00 0.00000000e+00 2.01100000e+03]\n",
      " [7.58051459e-01 1.00000000e+00 1.40000000e+01 2.00900000e+03]]\n",
      "Data was split into features and labels.\n",
      "[-0.04983157 -0.51736349  0.53586376 -1.47310915  1.        ]\n",
      "9.76824903992839\n",
      "9.97479039743095\n",
      "9.623479896770487\n",
      "9.703839468305086\n",
      "9.68000040753899\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def custom_cross_validation(cv, x_train, y_train, model):\n",
    "    pie_len = len(x_train) / cv\n",
    "    total_len = len(x_train)\n",
    "    for i in range(0, cv):\n",
    "        b = int(i * pie_len)\n",
    "        e = int(b + pie_len)\n",
    "        train_range = np.r_[0:b, e:total_len]\n",
    "        test_range = np.r_[b:e]\n",
    "        model.fit(x_train[train_range], y_train[train_range])\n",
    "        predictions = model.predict(x_train[test_range])\n",
    "\n",
    "        rmse = mean_squared_error(y_train[test_range], predictions, squared=False)\n",
    "\n",
    "        print(rmse)\n",
    "\n",
    "selectedFeatures = ['distance', 'passenger_count', 'pickup_hour', 'pickup_year']\n",
    "\n",
    "X = training.loc[:, selectedFeatures].values\n",
    "y = training.iloc[:, training.columns == 'fare_amount'].values.ravel()\n",
    "\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "print(\"Data was split into features and labels.\")\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X) \n",
    "\n",
    "# the one col is only added after scaling, these are the constant weights we need for regression\n",
    "X = np.insert(X, X.shape[1], np.ones(X.shape[0]), axis=1)\n",
    "\n",
    "print(X[0])\n",
    "\n",
    "regr = LinearRegression();\n",
    "\n",
    "custom_cross_validation(5, X, y, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f0a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b14644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1213203435596424\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error([12.5, 11], [12.5, 14], squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61311a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
