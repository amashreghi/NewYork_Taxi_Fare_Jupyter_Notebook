{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e395d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML\\\\NewYorkTaxi_Jupyter'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2042a317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2009-06-15 17:26:21 UTC\n",
      "1    2010-01-05 16:52:16 UTC\n",
      "2    2011-08-18 00:35:00 UTC\n",
      "3    2012-04-21 04:30:42 UTC\n",
      "4    2010-03-09 07:51:00 UTC\n",
      "Name: pickup_datetime, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training = pd.read_csv('data/train.csv', nrows=100)\n",
    "test = pd.read_csv('data/test.csv', nrows=100)\n",
    "\n",
    "pd.set_option('display.max_columns',10)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "print(training.pickup_datetime.head())\n",
    "type(training)\n",
    "\n",
    "training.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f388f2c",
   "metadata": {},
   "source": [
    "We need to write a custom function for cross validation.\n",
    "\n",
    "Ideas:\n",
    "0- Check for empty and null values\n",
    "1- Simplify pick time to a number between 0 to 24 to just represent time of the day\n",
    "2- Come up with a new column that shows the distance. This may not be great since the exact locations may affect the length of the trip that could affect the fare.\n",
    "3- Check for linear correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9dd7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Read 10000 training rows\n",
      "Read 9914 test rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training = pd.read_csv('data/train.csv', on_bad_lines='skip') # TODO remove limit\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "print(\"Read \" + str(len(training)) + \" training rows\")\n",
    "print(\"Read \" + str(len(test)) + \" test rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a7d76",
   "metadata": {},
   "source": [
    "Now we break apart the pickup times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346dc52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Read 1000000 training rows\n",
      "Read 9914 test rows\n",
      "   fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "0          4.5        -73.844311        40.721319         -73.841610         40.712278   \n",
      "1         16.9        -74.016048        40.711303         -73.979268         40.782004   \n",
      "2          5.7        -73.982738        40.761270         -73.991242         40.750562   \n",
      "3          7.7        -73.987130        40.733143         -73.991567         40.758092   \n",
      "4          5.3        -73.968095        40.768008         -73.956655         40.783762   \n",
      "\n",
      "   passenger_count  pickup_year  pickup_month  pickup_hour  pickup_dayofweek  distance  one  \n",
      "0                1         2009             6           17                 0  1.030764    1  \n",
      "1                1         2010             1           16                 1  8.450134    1  \n",
      "2                2         2011             8            0                 3  1.389525    1  \n",
      "3                1         2012             4            4                 5  2.799270    1  \n",
      "4                1         2010             3            7                 1  1.999157    1  \n",
      "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
      "0        -73.973320        40.763805         -73.981430         40.743835                1   \n",
      "1        -73.986862        40.719383         -73.998886         40.739201                1   \n",
      "2        -73.982524        40.751260         -73.979654         40.746139                1   \n",
      "3        -73.981160        40.767807         -73.990448         40.751635                1   \n",
      "4        -73.966046        40.789775         -73.988565         40.744427                1   \n",
      "\n",
      "   pickup_year  pickup_month  pickup_hour  pickup_dayofweek  distance  one  \n",
      "0         2015             1           13                 1  2.323260    1  \n",
      "1         2015             1           13                 1  2.425353    1  \n",
      "2         2011            10           11                 5  0.618628    1  \n",
      "3         2012            12           21                 5  1.961033    1  \n",
      "4         2012            12           21                 5  5.387301    1  \n"
     ]
    }
   ],
   "source": [
    "from math import cos, asin, sqrt, pi\n",
    "import pandas as pd\n",
    "\n",
    "training = pd.read_csv('data/train.csv', on_bad_lines='skip', nrows=1000000) # TODO remove limit\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "print(\"Read \" + str(len(training)) + \" training rows\")\n",
    "print(\"Read \" + str(len(test)) + \" test rows\")\n",
    "\n",
    "def convert_pickup_times(df):\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC', errors='coerce')\n",
    "\n",
    "def break_pickup_times_to_components(df):\n",
    "    df['pickup_year'] = df['pickup_datetime'].dt.year\n",
    "    df['pickup_month'] = df['pickup_datetime'].dt.month\n",
    "    #df['pickup_day'] = df['pickup_datetime'].dt.day\n",
    "    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "    #df['pickup_minute'] = df['pickup_datetime'].dt.minute\n",
    "    #converting seconds sounds like an overkill so I won't use it\n",
    "    \n",
    "    #but day of week sounds like a predictive feature\n",
    "    df['pickup_dayofweek'] = df['pickup_datetime'].dt.dayofweek\n",
    "    \n",
    "    #remove the redundant col\n",
    "    df.drop(columns=['pickup_datetime'], inplace=True)\n",
    "    \n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return 12742 * asin(sqrt(a)) #2*R*asin...\n",
    "    \n",
    "def get_distance(row):\n",
    "    return distance(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude'])\n",
    "    \n",
    "def prepare(df):\n",
    "    convert_pickup_times(df)\n",
    "    break_pickup_times_to_components(df)\n",
    "    df.drop(columns=['key'], inplace=True)\n",
    "    df['distance'] = df.apply(get_distance, axis=1)\n",
    "    df['one'] = 1 #constant term needed for linear regression\n",
    "    \n",
    "prepare(training)\n",
    "prepare(test)\n",
    "\n",
    "pd.set_option('display.max_columns',16)\n",
    "pd.set_option('display.width', 100)\n",
    "print(training.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb4d1c",
   "metadata": {},
   "source": [
    "Since the number of cells with null value is relatively small and there's no null values in the test data we can just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2249805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "999990 rows\n"
     ]
    }
   ],
   "source": [
    "def check_for_nulls(df):\n",
    "    is_null = df.isnull()\n",
    "    row_has_null = is_null.any(axis=1)\n",
    "    print(row_has_null.sum())\n",
    "    \n",
    "check_for_nulls(training)\n",
    "check_for_nulls(test)\n",
    "\n",
    "training = training[~training.isnull().any(axis=1)]\n",
    "print(str(len(training)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc380a40",
   "metadata": {},
   "source": [
    "Now, we want to take a look at the correlations between different features and our target variable, i.e. fare amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c1f6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fare_amount          1.000000\n",
       "pickup_longitude     0.008744\n",
       "pickup_latitude     -0.007680\n",
       "dropoff_longitude    0.009644\n",
       "dropoff_latitude    -0.007629\n",
       "passenger_count      0.012818\n",
       "pickup_year          0.115828\n",
       "pickup_month         0.025104\n",
       "pickup_hour         -0.018935\n",
       "pickup_dayofweek     0.002676\n",
       "distance             0.024779\n",
       "one                       NaN\n",
       "Name: fare_amount, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[training.columns[:]].corr(method='pearson')[:]['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d6d33d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999990, 3)\n",
      "[[1.03076393 1.         1.        ]\n",
      " [8.4501336  1.         1.        ]\n",
      " [1.38952523 2.         1.        ]\n",
      " ...\n",
      " [1.76174086 5.         1.        ]\n",
      " [1.84268322 1.         1.        ]\n",
      " [0.75805146 1.         1.        ]]\n",
      "Data was split into features and labels.\n",
      "9.834562013554319\n",
      "10.043780895073922\n",
      "9.692561576148192\n",
      "9.77003193354581\n",
      "9.745577734651624\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def custom_cross_validation(cv, x_train, y_train, model):\n",
    "    pie_len = len(x_train) / cv\n",
    "    total_len = len(x_train)\n",
    "    for i in range(0, cv):\n",
    "        b = int(i * pie_len)\n",
    "        e = int(b + pie_len)\n",
    "        train_range = np.r_[0:b, e:total_len]\n",
    "        test_range = np.r_[b:e]\n",
    "        model.fit(x_train[train_range], y_train[train_range])\n",
    "        predictions = model.predict(x_train[test_range])\n",
    "\n",
    "        rmse = mean_squared_error(y_train[test_range], predictions, squared=False)\n",
    "\n",
    "        print(rmse)\n",
    "\n",
    "selectedFeatures = ['distance', 'passenger_count', 'one']\n",
    "\n",
    "X = training.loc[:, selectedFeatures].values\n",
    "y = training.iloc[:, training.columns == 'fare_amount'].values.ravel()\n",
    "\n",
    "print(X.shape)\n",
    "print(X)\n",
    "\n",
    "print(\"Data was split into features and labels.\")\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "regr = LinearRegression();\n",
    "\n",
    "custom_cross_validation(5, X, y, regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f0a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b14644d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
